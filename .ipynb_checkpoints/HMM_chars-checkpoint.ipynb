{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "#opening the tweet data files\n",
    "with open('condensed_2011.json') as data_file:    \n",
    "    data2011 = json.load(data_file)\n",
    "with open('condensed_2012.json') as data_file:    \n",
    "    data2012 = json.load(data_file)\n",
    "with open('condensed_2013.json') as data_file:    \n",
    "    data2013 = json.load(data_file)\n",
    "with open('condensed_2014.json') as data_file:    \n",
    "    data2014 = json.load(data_file)\n",
    "with open('condensed_2015.json') as data_file:    \n",
    "    data2015 = json.load(data_file)\n",
    "with open('condensed_2016.json') as data_file:    \n",
    "    data2016 = json.load(data_file)\n",
    "with open('condensed_2017.json') as data_file:    \n",
    "    data2017 = json.load(data_file)\n",
    "\n",
    "#storing the data in a single variable\n",
    "tweets=[]\n",
    "for i in range(len(data2011)-1):\n",
    "    tweets.append(data2011[i][\"text\"])\n",
    "for i in range(len(data2012)-1):\n",
    "    tweets.append(data2012[i][\"text\"])\n",
    "for i in range(len(data2013)-1):\n",
    "    tweets.append(data2013[i][\"text\"])\n",
    "for i in range(len(data2014)-1):\n",
    "    tweets.append(data2014[i][\"text\"])\n",
    "for i in range(len(data2015)-1):\n",
    "    tweets.append(data2015[i][\"text\"])\n",
    "for i in range(len(data2016)-1):\n",
    "    tweets.append(data2016[i][\"text\"])\n",
    "for i in range(len(data2017)-1):\n",
    "    tweets.append(data2017[i][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#breaking the tweets into characters rather than words\n",
    "tweet_chars=[]\n",
    "for i in range(len(tweets)-1):\n",
    "    tweet_chars.append(list(tweets[i]))\n",
    "    \n",
    "#finding the unique set of characters used\n",
    "unique=[]\n",
    "for i in range(len(tweet_chars)-1):\n",
    "    for j in range(len(tweet_chars[i])-1):\n",
    "        if tweet_chars[i][j] not in unique:\n",
    "            unique.append(tweet_chars[i][j])\n",
    "\n",
    "unique[unique.index(' ')]=unique[0]\n",
    "unique[0]=' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob=np.zeros(len(unique))\n",
    "for i in range(len(tweet_chars)-1):\n",
    "    for k in range(len(tweet_chars[i])):\n",
    "        for j in range(len(unique)-1):\n",
    "            if tweet_chars[i][k]==unique[j]:\n",
    "                prob[j]=prob[j]+1\n",
    "prob=prob/len(tweet_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Switching from characters to integers\n",
    "numeric_map=range(1,np.size(unique))\n",
    "index=0\n",
    "max_chars=140\n",
    "hid_states=np.zeros(np.size(tweet_chars)*max_chars)\n",
    "for i in range(len(tweet_chars)):\n",
    "    for k in range(len(tweet_chars[i])):\n",
    "        for j in range(len(unique)):\n",
    "            if tweet_chars[i][k]==unique[j]:\n",
    "                hid_states[index]=j\n",
    "                index=index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hid=np.zeros((np.size(tweets)-1,max_chars))\n",
    "for i in range(np.size(tweets)-1):\n",
    "    for j in range(len(tweets[i])-1):\n",
    "        if len(tweets[i])<140:\n",
    "            hid[i,j]=hid_states[i+i*j]\n",
    "\n",
    "hid=hid[1:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hmmlearn import base\n",
    "\n",
    "lengths=np.zeros(len(tweet_chars))\n",
    "for i in range(len(tweet_chars)):\n",
    "    lengths[i]=len(tweet_chars[i])\n",
    "ave_len=int(lengths.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "from hmmlearn import base\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "model=hmm.GaussianHMM(n_components=100,n_iter=5);\n",
    "model.fit(hid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'i', 'm', '$', '%', '=', 'M', '―', '“', 'h', 'q', 'P', 'i', 'i', ':', '%', 's', 'U', '\\xa0', 'i', 'G', '$', 'C', '9', ')', 'h', ':', 'i', 'M', '%', '%', 'u', '”', 's', '\\xa0', '4', '4', '2', 's', 's', 'M', '=', '$', '3', '“', '8', '%', 's', 'X', '$', 'h', 'A', '0', '“', 'U', '2']\n"
     ]
    }
   ],
   "source": [
    "x,state_seq=model.sample(n_samples=np.random.randint(0,max(lengths)));\n",
    "seq=[]\n",
    "for i in range(len(state_seq)):\n",
    "    seq.append(unique[state_seq[i]])\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{', 'W', 's', 'ø', 'I', 'ō', 'd', 'p', 'N', 'g', 'r', 'e', 'C', 'N', 'p', 'ō', 'o', 'C', 'p', 'T', 'v', 'l', 'S', 'f', 'd', 's', 'T', 'á', '-', 'p', 'D', 'm', 'e', 'ø', 'W', 'w', '☀', 'b', '(', ' ', 'd', 'A', '?', '.', 'f', 'h', 'á', 'k', 'ø', '👎', 'r', ' ', 'o', 'm', 'S', 'g', 'b', 'g', 'ō', ',', 'E', 'v', '🇮', 'ō', '🤖', 'd', 'w', 'E', 'M', 'U', 'o', 'n', '↔', 'K', '-', ' ', 'g', 'á', '🇨', '@', ' ', 'T', 'l', 'g', 'E', 'E', 't', '.', 'k', 'n', ',', 'ø', 'r', 'y', 'A', '@', 'p', 'T', 'm', 'A', '?', 'y', 't', 'b', 'h', 'r', 'A', '@', 't', 'P', '🤔', 's', 'C', 'v', ' ', '-', '🇨', 'N', 'E', 'o', ' ', 's', 'O', 'r', 'o', 'r', 'y', 'f', 'l', 'O', 'e', '🍑', 'k', 'P', 'M', 'G', '🇦', '🎬', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "state_seq=model._generate_sample_from_state(50);\n",
    "seq=[]\n",
    "for i in range(len(state_seq)):\n",
    "    #if(unique[int(state_seq[i])]!=' '):\n",
    "    seq.append(unique[int(state_seq[i])])\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
